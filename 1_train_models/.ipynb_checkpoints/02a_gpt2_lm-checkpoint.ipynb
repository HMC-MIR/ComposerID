{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a GPT-2 Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we train a GPT-2 language model on the IMSLP and/or target data.  This code can be used to train two different language models: (a) one that is trained on target data, and (b) one that is trained on IMSLP data and finetuned on target data.  For (a), you can stop at the end of the section entitled \"Train Language Model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from train_utils import plotLosses\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = Path('/home/dyang/.fastai/data/bscore_lm/bpe_data')\n",
    "bpe_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'imslp' # 'target' or 'imslp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_train_file = bpe_path/f'bpe_lm_{data_type}_train-augmented.txt'\n",
    "lm_valid_file = bpe_path/f'bpe_lm_{data_type}_valid-augmented.txt'\n",
    "tok_path = bpe_path/f'tokenizer_{data_type}'\n",
    "output_model_path = bpe_path/f'models/gpt2_train-{data_type}_lm-augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes from defaults:\n",
    "# vocab_size: 50257 -> 30000\n",
    "# n_positions: 1024 -> 514\n",
    "# n_ctx: 1024 -> 514\n",
    "# n_layer: 12 -> 6\n",
    "config = {\n",
    "    \"architectures\": [\n",
    "        \"GPT2LMHeadModel\"\n",
    "    ],\n",
    "    \"vocab_size\": 30000,\n",
    "    \"n_positions\": 514,\n",
    "    \"n_ctx\": 514,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_layer\": 6,\n",
    "    \"n_head\": 12,\n",
    "    \"resid_pdrop\": 0.1,\n",
    "    \"embd_pdrop\": 0.1,\n",
    "    \"attn_pdrop\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-5,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"summary_type\": \"cls_index\",\n",
    "    \"summary_use_proj\": True,\n",
    "    \"summary_activation\": None,\n",
    "    \"summary_proj_to_labels\": True,\n",
    "    \"summary_first_dropout\": 0.1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{tok_path}/config.json\", 'w') as fp:\n",
    "    json.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"\n",
    "python ./run_language_modeling.py\n",
    "--train_data_file {lm_train_file}\n",
    "--output_dir {output_model_path}\n",
    "--model_type gpt2\n",
    "--eval_data_file {lm_valid_file}\n",
    "--line_by_line\n",
    "--config_name {tok_path}\n",
    "--tokenizer_name {tok_path}\n",
    "--do_train\n",
    "--do_eval\n",
    "--evaluate_during_training\n",
    "--per_gpu_train_batch_size 16\n",
    "--per_gpu_eval_batch_size 16\n",
    "--learning_rate 1e-4\n",
    "--num_train_epochs 12\n",
    "--logging_steps 7180\n",
    "--save_steps 7180\n",
    "--seed 42\n",
    "--overwrite_output_dir\n",
    "--should_continue\n",
    "\"\"\".replace(\"\\n\", \" \")\n",
    "#--save_total_limit 2\n",
    "#--should_continue\n",
    "# target data: batch size 16, 204 steps per epoch, 12 epochs\n",
    "# imslp data: batch size 16, 7180 steps per epoch, ? epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {cmd} > train_lm_augmented.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you may need to run this in a bash shell with the appropriate virtual environment\n",
    "!./train_lm.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_model_path}/config.json\", 'w') as fp:\n",
    "    json.dump(config, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLosses(output_model_path/'eval_results.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Finetune Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section only applies for the LM trained on IMSLP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_models_path = bpe_path/'models/gpt2_train-imslp_finetune-target_lm-augmented'\n",
    "lm_train_file = bpe_path/'bpe_lm_target_train-augmented.txt'\n",
    "lm_valid_file = bpe_path/'bpe_lm_target_valid-augmented.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"\"\"\n",
    "python ./run_language_modeling.py\n",
    "--train_data_file {lm_train_file}\n",
    "--eval_data_file {lm_valid_file}\n",
    "--output_dir {finetuned_models_path}\n",
    "--model_type gpt2\n",
    "--line_by_line\n",
    "--model_name_or_path {output_model_path}\n",
    "--tokenizer_name {output_model_path}\n",
    "--do_train\n",
    "--do_eval\n",
    "--evaluate_during_training\n",
    "--per_gpu_train_batch_size 16\n",
    "--per_gpu_eval_batch_size 16\n",
    "--learning_rate 5e-5\n",
    "--num_train_epochs 12\n",
    "--logging_steps 204\n",
    "--save_steps 204\n",
    "--seed 42\n",
    "--overwrite_output_dir\n",
    "\"\"\".replace(\"\\n\", \" \")\n",
    "#--save_total_limit 2\n",
    "#--should_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {cmd} > train_lm-fine_tune.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you may need to run this in a bash shell with different virtual environment\n",
    "!./train_lm-fine_tune-debug.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLosses(finetuned_models_path/'eval_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
