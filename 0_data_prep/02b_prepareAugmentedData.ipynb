{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cleans and prepares the data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import glob\n",
    "import PIL\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out filler pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove filler pages that do not contain sheet music information (e.g. title, foreword).  The file filler.txt indicates which pages are filler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFillerList(filler_file, feat_dir):\n",
    "    d = {} # list of pages to remove\n",
    "    with open(filler_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) > 1:\n",
    "                relpath = parts[0] # e.g. Bach/00748\n",
    "                scoreID = os.path.basename(relpath) # e.g. 00748\n",
    "                removeField = parts[1].strip('\"') # e.g. \"0,1,-2,-1\" or \"r\" or \"rl\"\n",
    "                numPages = getNumPages(relpath, feat_dir)\n",
    "                if removeField == 'r' or removeField == 'rl': # remove all pages\n",
    "                    for pkl_file in glob.glob('{}/{}/*.pkl'.format(feat_dir, parts[0])):\n",
    "                        pageID = os.path.splitext(os.path.basename(pkl_file))[0] # e.g. 00822-3\n",
    "                        d[pageID] = 1\n",
    "                else:\n",
    "                    for pageNumStr in removeField.split(','):\n",
    "                        pageNum = int(pageNumStr)\n",
    "                        if pageNum < 0:\n",
    "                            pageID = '{}-{}'.format(scoreID, numPages + pageNum)\n",
    "                            d[pageID] = 1\n",
    "                        else:\n",
    "                            pageID = '{}-{}'.format(scoreID, pageNum)\n",
    "                            d[pageID] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumPages(relpath, indir):\n",
    "    numPages = len(glob.glob('{}/{}/*.pkl'.format(indir, relpath)))\n",
    "    return numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonFillerFeatures(filler_file, feat_dir):\n",
    "    '''\n",
    "    Collect bootleg score features from all pages that are (a) not filler and (b) have a valid \n",
    "    bootleg score matrix.\n",
    "    '''\n",
    "    \n",
    "    filler = getFillerList(filler_file, feat_dir)\n",
    "    feats = {}\n",
    "    \n",
    "    for pieceDir in glob.glob('{}/*/*/'.format(feat_dir)): # e.g. score_feat/Bach/00748/\n",
    "        \n",
    "        pieceID = pieceDir.split('/')[-2]\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        accum = [] # features from all pages in this score (or None if filler/no features extracted)\n",
    "        numPages = len(glob.glob('{}/*.pkl'.format(pieceDir)))\n",
    "        \n",
    "        for i in range(numPages):\n",
    "            \n",
    "            pkl_file = '{}/{}-{}.pkl'.format(pieceDir, pieceID, i)\n",
    "            pageID = '{}-{}'.format(pieceID, i) # e.g. 00748-2\n",
    "            if pageID in filler: # filler page, skip\n",
    "                accum.append(None)\n",
    "                continue\n",
    "            with open(pkl_file, 'rb') as f:\n",
    "                bscore = pickle.load(f)['bscore']\n",
    "            if bscore is None: # if None, no features were computed\n",
    "                accum.append(None)\n",
    "            else:\n",
    "                accum.append(bscore == 1) # convert from float to bool to compress memory\n",
    "        \n",
    "        if len(accum) > 0:\n",
    "            feats[pieceDir] = accum\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_file = '../cfg_files/filler.txt'\n",
    "score_feat_dir = '../score_feat'\n",
    "feats = getNonFillerFeatures(filler_file, score_feat_dir) # key: pieceDir, value: list of bscore matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureStats(feats):\n",
    "    \n",
    "    # count number of features per page\n",
    "    featsPerPage = []\n",
    "    for pieceDir in feats:\n",
    "        for elem in feats[pieceDir]:\n",
    "            if elem is not None:\n",
    "                featsPerPage.append(elem.shape[1])\n",
    "    featsPerPage = np.array(featsPerPage)\n",
    "    printStats(featsPerPage, \"Number of Features Per Page\")\n",
    "    \n",
    "    # plot histogram\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.hist(featsPerPage, bins=100)\n",
    "    plt.xlabel('Number of Events In Single Page')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # count total number of pages by composer\n",
    "    pages = {}\n",
    "    for pieceDir in feats: # e.g. score_feat/Bach/00748/\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        if composer not in pages:\n",
    "            pages[composer] = 0\n",
    "        pages[composer] += len([1 for elem in feats[pieceDir] if elem is not None])\n",
    "    pageCnts = [pages[composer] for composer in pages]\n",
    "    composers = [composer[0:5] for composer in pages]\n",
    "    printStats(pageCnts, \"Total Number of Pages by Composer\")\n",
    "    \n",
    "    # plot histogram\n",
    "    x_pos = np.arange(len(pageCnts))\n",
    "    plt.bar(x_pos, pageCnts)\n",
    "    plt.xticks(x_pos, composers)\n",
    "    plt.ylabel('Total # Pages')\n",
    "    plt.show()\n",
    "    \n",
    "    # count total number of note events by composer\n",
    "    noteEvents = {}\n",
    "    for pieceDir in feats: # e.g. score_feat/Bach/00748/\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        if composer not in noteEvents:\n",
    "            noteEvents[composer] = 0\n",
    "        for elem in feats[pieceDir]:\n",
    "            if elem is not None:\n",
    "                noteEvents[composer] += elem.shape[1]\n",
    "    noteEventCnts = [noteEvents[composer] for composer in noteEvents]\n",
    "    printStats(noteEventCnts, \"Total Number of Note Events by Composer\")\n",
    "    \n",
    "    # plot histogram\n",
    "    x_pos = np.arange(len(composers))\n",
    "    plt.bar(x_pos, noteEventCnts)\n",
    "    plt.xticks(x_pos, composers)\n",
    "    plt.ylabel('Total # Note Events')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStats(arr, title = None):\n",
    "    if title:\n",
    "        print(title)\n",
    "    print('Mean: {}'.format(np.mean(arr)))\n",
    "    print('Std: {}'.format(np.std(arr)))\n",
    "    print('Min: {}'.format(np.min(arr)))\n",
    "    print('Max: {}'.format(np.max(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeatureStats(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a wide variation in the number of features per page and a significant class imbalance in the data.  We will define a proxy task which tries to classify short fixed-length chunks of bootleg score features, and resample the classes to ensure class balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train, Validation, & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we separate the data into train, validation, & test sets.  We split the data by piece (as opposed to page) to ensure total separation.  This data corresponds to the original task of classifying a single page of sheet music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainValidTest(d, train=.6, validation=.2, test=.2, savefile = None):\n",
    "    \n",
    "    # shuffle\n",
    "    assert(train + validation + test == 1.0)\n",
    "    np.random.seed(0)\n",
    "    pieceDirs = list(d.keys())\n",
    "    np.random.shuffle(pieceDirs)\n",
    "    \n",
    "    # split\n",
    "    breakpt1 = int(len(pieceDirs) * train)\n",
    "    breakpt2 = breakpt1 + int(len(pieceDirs) * validation)\n",
    "    pieceDirs_train = pieceDirs[0:breakpt1]\n",
    "    pieceDirs_valid = pieceDirs[breakpt1:breakpt2]\n",
    "    pieceDirs_test = pieceDirs[breakpt2:]\n",
    "    \n",
    "    # save\n",
    "    d_train = getDataSubset(d, pieceDirs_train)\n",
    "    d_valid = getDataSubset(d, pieceDirs_valid)\n",
    "    d_test = getDataSubset(d, pieceDirs_test)\n",
    "    if savefile:\n",
    "        saveToPickle([d, pieceDirs_train, pieceDirs_valid, pieceDirs_test], savefile)\n",
    "    \n",
    "    return d_train, d_valid, d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSubset(dAll, toKeep):\n",
    "    dSubset = {}\n",
    "    for pieceDir in toKeep:\n",
    "        print(dAll[pieceDir][0])\n",
    "        dSubset[pieceDir] = dAll[pieceDir]\n",
    "    return dSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToPickle(d, outfile):\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickle(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pages_file = '{}/data.pages.pkl'.format(score_feat_dir)\n",
    "d_train, d_valid, d_test = splitTrainValidTest(feats, train=.6, validation=.2, test=.2, savefile=save_pages_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we prepare the data for the proxy task, which assumes a fixed-length (L=64) chunk of bootleg features.  To ensure class balance, we randomly sample the same number of chunks from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComposer2IndexMapping(feat_dir):\n",
    "    composers = []\n",
    "    for composerDir in sorted(glob.glob('{}/*/'.format(feat_dir))):\n",
    "        composer = composerDir.split('/')[-2]\n",
    "        composers.append(composer)\n",
    "    c_to_i = {c:i for i, c in enumerate(composers)}\n",
    "    \n",
    "    return c_to_i, composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunkedData_uniformSampling(d, chunkSize, c_to_i):\n",
    "    '''\n",
    "    Uniform sampling with 50% overlap.  Doesn't keep page location info.\n",
    "    '''\n",
    "    frags = []\n",
    "    labels = []\n",
    "    pieceDir2idxRange = {}\n",
    "    for pieceDir in d:\n",
    "        merged = np.hstack(d[pieceDir])\n",
    "        composerIdx = c_to_i[pieceDir.split('/')[-3]]\n",
    "        startChunkIdx = len(frags)\n",
    "        for startIdx in range(0, merged.shape[1], chunkSize // 2):\n",
    "            endIdx = startIdx + chunkSize\n",
    "            if endIdx <= merged.shape[1]:\n",
    "                frags.append(merged[:,startIdx:endIdx])\n",
    "                labels.append(composerIdx)\n",
    "        endChunkIdx = len(frags)\n",
    "        pieceDir2idxRange[pieceDir] = (startChunkIdx, endChunkIdx)\n",
    "    frags = np.array(frags)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return frags, labels, pieceDir2idxRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunkedData_balanced(d, chunkSize, samplesPerComposer, composers):\n",
    "    '''\n",
    "    Samples the same number of windows from each composer to avoid class imbalance.\n",
    "    Also keeps the page location for debugging and error analysis.\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    frags = []\n",
    "    labels = []\n",
    "    pagelocs = [] # list of (pieceDir, startPage, endPage) tuples\n",
    "    \n",
    "    for composerIdx, composer in enumerate(composers):\n",
    "        samples, locinfo = sampleFromComposer(d, chunkSize, samplesPerComposer, composer)\n",
    "        frags.extend(samples)\n",
    "        labels.extend([composerIdx] * samplesPerComposer)\n",
    "        pagelocs.extend(locinfo)\n",
    "        \n",
    "    frags = np.array(frags)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # shuffle indices\n",
    "    shuffled_idxs = np.arange(len(labels))\n",
    "    np.random.shuffle(shuffled_idxs)\n",
    "    frags = frags[shuffled_idxs]\n",
    "    labels = labels[shuffled_idxs]\n",
    "    pagelocs = [pagelocs[i] for i in shuffled_idxs]\n",
    "    \n",
    "    return frags, labels, pagelocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleFromComposer(d, chunkSize, N, composer):\n",
    "    '''\n",
    "    Samples N windows of length chunkSize from the bootleg score data from the specified composer.\n",
    "    Returns the sampled data, along with piece & page location for error analysis.\n",
    "    '''\n",
    "    \n",
    "    # generate list of all valid sample locations\n",
    "    allLocs = [] # list of (pieceNum, offset) tuples\n",
    "    validPieceDirs = [] # pieceDirs that correspond to this composer\n",
    "    bscores = [] # list of bootleg scores from each piece\n",
    "    for pieceDir in d:\n",
    "        if pieceDir.split('/')[-3] == composer:\n",
    "            pieceNum = len(validPieceDirs)\n",
    "            validPageFeats = [elem for elem in d[pieceDir] if elem is not None]\n",
    "            if len(validPageFeats) == 0:\n",
    "                continue\n",
    "            merged = np.hstack(validPageFeats)\n",
    "            for col_offset in range(merged.shape[1] - chunkSize + 1):\n",
    "                allLocs.append((pieceNum, col_offset))\n",
    "            validPieceDirs.append(pieceDir)\n",
    "            bscores.append(merged)\n",
    "\n",
    "    # generate samples\n",
    "    frags = []\n",
    "    pagelocs = []\n",
    "    sample_idxs = np.random.choice(len(allLocs), N)\n",
    "    for sample_idx in sample_idxs:\n",
    "        pieceNum, offset = allLocs[sample_idx]\n",
    "        pieceDir = validPieceDirs[pieceNum]\n",
    "        frag = bscores[pieceNum][:, offset:offset+chunkSize]\n",
    "        locStart = determinePageLocation(d, pieceDir, offset) # (pageLocFloat, pageNum, pageOffset)\n",
    "        locEnd = determinePageLocation(d, pieceDir, offset + chunkSize - 1)\n",
    "        frags.append(frag)\n",
    "        pagelocs.append((pieceDir, locStart, locEnd))\n",
    "\n",
    "    return frags, pagelocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinePageLocation(d, pieceDir, offset):\n",
    "    '''\n",
    "    Determines the page location of the given bootleg score column offset.  The page location\n",
    "    is calculated as a floating point number.\n",
    "    '''\n",
    "    accum = 0\n",
    "    pageloc = (-1, -1, -1) # (pageLocFloat, pageNum, pageOffset)\n",
    "    for pageIdx, elem in enumerate(d[pieceDir]):\n",
    "        if elem is not None:\n",
    "            numEvents = elem.shape[1]\n",
    "            if accum + numEvents > offset:\n",
    "                frac = (offset - accum) / (numEvents - 1) * 1.0\n",
    "                pageFloat = pageIdx + frac\n",
    "                pageloc = (pageFloat, pageIdx, offset - accum) \n",
    "                break\n",
    "            accum += numEvents\n",
    "    return pageloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer2idx, composers = getComposer2IndexMapping(score_feat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSize = 64 # pick a chunk size a bit larger than 64 to allow for additional data augmentation\n",
    "samplesPerComposer = 3600\n",
    "X_train, y_train, pageinfo_train = getChunkedData_balanced(d_train, chunkSize, samplesPerComposer, composers)\n",
    "X_valid, y_valid, pageinfo_valid = getChunkedData_balanced(d_valid, chunkSize, samplesPerComposer//3, composers)\n",
    "X_test, y_test, pageinfo_test = getChunkedData_balanced(d_test, chunkSize, samplesPerComposer//3, composers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chunks_file = f'{score_feat_dir}/data.chunks{chunkSize}.pkl'\n",
    "saveToPickle([X_train, y_train, pageinfo_train, X_valid, y_valid, pageinfo_valid, X_test, y_test, pageinfo_test], save_chunks_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSizes = [64, 128, 256]\n",
    "shiftss = [[-3,-2,-1,0,1,2,3],[-2,-1,0,1,2],[-1,0,1]]\n",
    "for shifts in shiftss:\n",
    "    for chunkSize in chunkSizes:\n",
    "        score_feat_dir = 'score_feat'\n",
    "        save_chunks_file = f'{score_feat_dir}/data.chunks{chunkSize}.pkl'\n",
    "        out_chunks_file = f'{score_feat_dir}/augment/data.chunks{chunkSize}.augment{len(shifts)}.pkl'\n",
    "        print(out_chunks_file)\n",
    "        [X_train, y_train, pageinfo_train, X_valid, y_valid, pageinfo_valid, X_test, y_test, pageinfo_test] = loadPickle(save_chunks_file)\n",
    "        X_train_new = []\n",
    "        y_train_new = []\n",
    "        pageinfo_train_new = []\n",
    "        X_valid_new = []\n",
    "        y_valid_new = []\n",
    "        pageinfo_valid_new = []\n",
    "        X_test_new = []\n",
    "        y_test_new = []\n",
    "        pageinfo_test_new = []\n",
    "        for idx, bscore in enumerate(X_train):\n",
    "            shift_bscores = editBscoreByShifts(bscore, shifts)\n",
    "            for shift_bscore in shift_bscores:\n",
    "                X_train_new.append(shift_bscore)\n",
    "                y_train_new.append(y_train[idx])\n",
    "                pageinfo_train_new.append(pageinfo_train[idx])\n",
    "        for idx, bscore in enumerate(X_valid):\n",
    "            shift_bscores = editBscoreByShifts(bscore, shifts)\n",
    "            for shift_bscore in shift_bscores:\n",
    "                X_valid_new.append(shift_bscore)\n",
    "                y_valid_new.append(y_valid[idx])\n",
    "                pageinfo_valid_new.append(pageinfo_valid[idx])\n",
    "        for idx, bscore in enumerate(X_test):\n",
    "            shift_bscores = editBscoreByShifts(bscore, shifts)\n",
    "            for shift_bscore in shift_bscores:\n",
    "                X_test_new.append(shift_bscore)\n",
    "                y_test_new.append(y_valid[idx])\n",
    "                pageinfo_test_new.append(pageinfo_test[idx])\n",
    "\n",
    "        X_train_new = np.array(X_train_new)\n",
    "        y_train_new = np.array(y_train_new)\n",
    "        pageinfo_train_new = np.array(pageinfo_train_new)\n",
    "        X_valid_new = np.array(X_valid_new)\n",
    "        y_valid_new = np.array(y_valid_new)\n",
    "        pageinfo_valid_new = np.array(pageinfo_valid_new)\n",
    "        X_test_new = np.array(X_test_new)\n",
    "        y_test_new = np.array(y_test_new)\n",
    "        pageinfo_test_new = np.array(pageinfo_test_new)\n",
    "\n",
    "        write_file = [X_train_new, y_train_new, pageinfo_train_new,\n",
    "                      X_valid_new, y_valid_new, pageinfo_valid_new,\n",
    "                      X_test_new, y_test_new, pageinfo_test_new]\n",
    "        with open(out_chunks_file, 'wb') as f:\n",
    "            pickle.dump(write_file, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the bootleg score data chunks against the original png images to verify that the data has been processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSize = 64\n",
    "score_feat_dir = '../score_feat'\n",
    "save_chunks_file = f'{score_feat_dir}/augment/data.chunks{chunkSize}.augment7.pkl'\n",
    "[X_train, y_train, pageinfo_train, X_valid, y_valid, pageinfo_valid, X_test, y_test, pageinfo_test] = loadPickle(save_chunks_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeBootlegScore(bs, lines = [13, 15, 17, 19, 21, 35, 37, 39, 41, 43]):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(1 - bs, cmap = 'gray', origin = 'lower')\n",
    "    for l in range(1, bs.shape[0], 2):\n",
    "        plt.axhline(l, c = 'grey')\n",
    "    for l in lines:\n",
    "        plt.axhline(l, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrespondingImage(tup, png_dir = 'data/png', showNext = False):\n",
    "    pieceDir, startLoc, endLoc = tup\n",
    "    composer = pieceDir.split('/')[-3]\n",
    "    pieceID = pieceDir.split('/')[-2]\n",
    "    startpage = startLoc[1]\n",
    "    if showNext:\n",
    "        startpage += 1\n",
    "    pngfile = '{}/{}/{}/{}-{}.png'.format(png_dir, composer, pieceID, pieceID, startpage)\n",
    "    im = PIL.Image.open(pngfile)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "visualizeBootlegScore(X_train[i])\n",
    "X_train[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageinfo_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = getCorrespondingImage(pageinfo_train[i], showNext = True)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below, we will prepare the data for use with the fastai library.  This is adapted from the fast.ai [ULMFit tutorial](https://github.com/fastai/course-nlp/blob/master/nn-vietnamese.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Config.data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'bscore_lm'\n",
    "path = data_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Language Model Databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the bootleg score features into string representations of decimal integers.  Generate one document per pdf. Augmentation type is sample pitch shift augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_docs = path/'docs_target-augmented'\n",
    "path_docs.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBinaryToInt(X):\n",
    "    mask = np.power(2, np.arange(X.shape[0])).reshape((1,-1))\n",
    "    ints = np.squeeze(mask @ X).astype(np.uint64)\n",
    "    return list(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValid(matrixList):\n",
    "    for elem in matrixList:\n",
    "        if elem is None:\n",
    "            continue\n",
    "        else:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editBscoreByShifts(bscore, shifts):\n",
    "    # positive = shift right (higher)\n",
    "    augmented_bscores = []\n",
    "    for shift in shifts:\n",
    "        bscore1 = copy.copy(bscore)\n",
    "        if bscore.shape[0] != 62 or bscore.shape[1] == 0:\n",
    "            continue\n",
    "        LH = bscore1[:28,:]\n",
    "        RH = bscore1[28:,:]\n",
    "        shift_LH = np.zeros((LH.shape[0]+abs(shift),LH.shape[1]))\n",
    "        shift_RH = np.zeros((RH.shape[0]+abs(shift),RH.shape[1]))\n",
    "        if shift < 0:\n",
    "            shift_LH[:-abs(shift),:] = LH\n",
    "            shift_RH[:-abs(shift),:] = RH\n",
    "            new_LH = shift_LH[-LH.shape[0]:,:]\n",
    "            new_RH = shift_RH[-RH.shape[0]:,:]\n",
    "        else:\n",
    "            shift_LH[abs(shift):,:] = LH\n",
    "            shift_RH[abs(shift):,:] = RH\n",
    "            new_LH = shift_LH[:LH.shape[0],:]\n",
    "            new_RH = shift_RH[:RH.shape[0],:]\n",
    "        bscore1 = np.vstack((new_LH,new_RH))\n",
    "        augmented_bscores.append(bscore1)\n",
    "    return augmented_bscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_bscore_target(d,shifts):\n",
    "    output_bscores = [[] for i in range(len(shifts))]\n",
    "    for page in d:\n",
    "        if page is None:\n",
    "            continue\n",
    "        bscores = editBscoreByShifts(page, shifts)\n",
    "        for idx, new_page in enumerate(bscores):\n",
    "            output_bscores[idx].append(new_page)\n",
    "    return output_bscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBootlegStringFiles(bscore_feats_file, outdir):\n",
    "    '''\n",
    "    Converts the bootleg score features to string decimal representation, and writes them\n",
    "    to text files in the specified directory.\n",
    "    '''\n",
    "    \n",
    "    with open(bscore_feats_file, 'rb') as f:\n",
    "        bscore_feats = pickle.load(f)[0]\n",
    "                \n",
    "    for pieceDir in bscore_feats: # e.g. score_feat/Bach/00748/\n",
    "        pid = pieceDir.split('/')[-2] # 00748\n",
    "        if isValid(bscore_feats[pieceDir]): # has at least one valid page of features\n",
    "            shifts = [-3,-2,-1,0,1,2,3]\n",
    "            bscores = augment_bscore_target(bscore_feats[pieceDir], shifts)\n",
    "            for idx, shift in enumerate(shifts):\n",
    "                outfile = outdir/ (pid +f'({shift}).txt')\n",
    "                print(outfile)\n",
    "                \n",
    "                with open(outfile,'w') as fout:\n",
    "                    for binaryMatrix in bscores[idx]:\n",
    "                        if binaryMatrix is not None:\n",
    "                            ints = convertBinaryToInt(binaryMatrix)\n",
    "                            pageStr = ' '.join([str(i) for i in ints])\n",
    "                            fout.write(pageStr)\n",
    "                            fout.write('\\n\\n')\n",
    "                    fout.write('</doc>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bscore2textfile(infile, outfile, min_thresh = 100):\n",
    "    '''\n",
    "    Converts a bootleg score .pkl file to text and writes to the specified output file.\n",
    "    '''\n",
    "    with open(infile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    for idx,shift in enumerate(shifts):\n",
    "        new_ext = os.path.splitext(os.path.basename(outfile))[0]+'({})'.format(shift)+os.path.splitext(os.path.basename(outfile))[1]\n",
    "        outfile_shift ='{}/{}'.format(os.path.dirname(outfile), new_ext)\n",
    "        with open(outfile_shift, 'w') as fout:\n",
    "            d = bscores[idx]\n",
    "            for l in d: # each page\n",
    "                if len(l) > min_thresh: # to avoid filler pages\n",
    "                    pageStr = ' '.join([str(i) for i in l])\n",
    "                    fout.write(pageStr)\n",
    "                    fout.write('\\n\\n')\n",
    "            fout.write('</doc>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateBootlegStringFiles(save_pages_file, path_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_docs.ls()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicTokenizer = Tokenizer(pre_rules=[], post_rules=[])\n",
    "lm_target_data = (TextList.from_folder(path_docs, processor=[OpenFileProcessor(), TokenizeProcessor(tokenizer=basicTokenizer), NumericalizeProcessor()])\n",
    "            .split_by_rand_pct(0.1, seed=42)\n",
    "            .label_for_lm()           \n",
    "            .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "lm_target_data.save(path/'lm_target_databunch-augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMSLP Language Model Databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but using the entire IMSLP piano bootleg score dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/HMC-MIR/piano_bootleg_scores.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imslp_bscores_filelist = 'imslp_bscores.list'\n",
    "!find piano_bootleg_scores/imslp_bootleg_dir-v1/ -name *.pkl > {imslp_bscores_filelist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_docs = path/'docs_imslp-augmented'\n",
    "path_docs.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeColumn(num):\n",
    "    col = []\n",
    "    for i in range(62):\n",
    "        col.insert(0,num%2)\n",
    "        num = int(num/2)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootlegHash(arr):\n",
    "    bitstring = \"\"\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i]==1:\n",
    "            bitstring+=\"1\"\n",
    "        else:\n",
    "            bitstring +=\"0\"\n",
    "    bitstring = bitstring+\"00\"\n",
    "    hashint = int(bitstring, 2)\n",
    "    hashint = np.uint64(hashint)\n",
    "    return hashint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_bscore(d,shifts):\n",
    "    output_bscores = [[] for i in range(len(shifts))]\n",
    "    \n",
    "    for page in d:\n",
    "        page_bscore = []\n",
    "        for column in page:\n",
    "            col = decodeColumn(column)\n",
    "            page_bscore.append(col)\n",
    "        page_bscore = np.array(page_bscore).T\n",
    "        bscores = editBscoreByShifts(page_bscore, shifts)\n",
    "        for idx, bscore in enumerate(bscores):\n",
    "            hashed_bscore = []\n",
    "            for col in bscore.T:\n",
    "                hashint = bootlegHash(col)\n",
    "                hashed_bscore.append(hashint)\n",
    "            output_bscores[idx].append(hashed_bscore)\n",
    "    return output_bscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editBscoreByShifts(bscore, shifts):\n",
    "    # positive = shift right (higher)\n",
    "    augmented_bscores = []\n",
    "    for shift in shifts:\n",
    "        bscore1 = copy.copy(bscore)\n",
    "        if bscore.shape[0] != 62 or bscore.shape[1] == 0:\n",
    "            continue\n",
    "        LH = bscore1[:28,:]\n",
    "        RH = bscore1[28:,:]\n",
    "        shift_LH = np.zeros((LH.shape[0]+abs(shift),LH.shape[1]))\n",
    "        shift_RH = np.zeros((RH.shape[0]+abs(shift),RH.shape[1]))\n",
    "        if shift < 0:\n",
    "            shift_LH[:-abs(shift),:] = LH\n",
    "            shift_RH[:-abs(shift),:] = RH\n",
    "            new_LH = shift_LH[-LH.shape[0]:,:]\n",
    "            new_RH = shift_RH[-RH.shape[0]:,:]\n",
    "        else:\n",
    "            shift_LH[abs(shift):,:] = LH\n",
    "            shift_RH[abs(shift):,:] = RH\n",
    "            new_LH = shift_LH[:LH.shape[0],:]\n",
    "            new_RH = shift_RH[:RH.shape[0],:]\n",
    "        bscore1 = np.vstack((new_LH,new_RH))\n",
    "        augmented_bscores.append(bscore1)\n",
    "    return augmented_bscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bscore2textfile(infile, outfile, min_thresh = 100):\n",
    "    '''\n",
    "    Converts a bootleg score .pkl file to text and writes to the specified output file.\n",
    "    '''\n",
    "    with open(infile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    shifts = [-3,-2,-1,0,1,2,3]\n",
    "    bscores = augment_bscore(d, shifts)\n",
    "    \n",
    "    for idx,shift in enumerate(shifts):\n",
    "        new_ext = os.path.splitext(os.path.basename(outfile))[0]+'({})'.format(shift)+os.path.splitext(os.path.basename(outfile))[1]\n",
    "        outfile_shift ='{}/{}'.format(os.path.dirname(outfile), new_ext)\n",
    "        with open(outfile_shift, 'w') as fout:\n",
    "            d = bscores[idx]\n",
    "            for l in d: # each page\n",
    "                if len(l) > min_thresh: # to avoid filler pages\n",
    "                    pageStr = ' '.join([str(i) for i in l])\n",
    "                    fout.write(pageStr)\n",
    "                    fout.write('\\n\\n')\n",
    "            fout.write('</doc>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imslp2text(filelist, outdir):\n",
    "    with open(filelist, 'r') as f:\n",
    "        for line in f:\n",
    "            bscorefile = line.strip() # path/to/dest/283513.pkl\n",
    "            fileid = os.path.splitext(os.path.basename(bscorefile))[0] # e.g. 283513\n",
    "            outfile = outdir/f'{fileid}.txt'\n",
    "            bscore2textfile(bscorefile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imslp2text(imslp_bscores_filelist, path_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_docs.ls()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicTokenizer = Tokenizer(pre_rules=[], post_rules=[])\n",
    "lm_imslp_data = (TextList.from_folder(path_docs, processor=[OpenFileProcessor(), TokenizeProcessor(tokenizer=basicTokenizer), NumericalizeProcessor()])\n",
    "            .split_by_rand_pct(0.1, seed=42)\n",
    "            .label_for_lm()           \n",
    "            .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "lm_imslp_data.save(path/'lm_imslp_databunch-augmented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the train.csv, valid.csv, and test.csv files for the proxy classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBootlegCSVFiles(bscore_feats_file, idx2composer, outfile_train, outfile_valid, outfile_test):\n",
    "    '''\n",
    "    Generates train.csv and test.csv from bootleg score fragments.\n",
    "    '''\n",
    "    with open(bscore_feats_file, 'rb') as f:\n",
    "        X_train, y_train, _, X_valid, y_valid, _, X_test, y_test, _ = pickle.load(f)\n",
    "        \n",
    "    y_train = [idx2composer[idx] for idx in y_train]\n",
    "    y_valid = [idx2composer[idx] for idx in y_valid]\n",
    "    y_test = [idx2composer[idx] for idx in y_test]\n",
    "    \n",
    "    with open(outfile_train, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for i in range(X_train.shape[0]):\n",
    "            ints = convertBinaryToInt(X_train[i,:,:])\n",
    "            pageStr = ' '.join([str(i) for i in ints])\n",
    "            fout.write(f'{y_train[i]},')\n",
    "            fout.write(pageStr)\n",
    "            fout.write('\\n')\n",
    "\n",
    "    with open(outfile_valid, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for i in range(X_valid.shape[0]):\n",
    "            ints = convertBinaryToInt(X_valid[i,:,:])\n",
    "            pageStr = ' '.join([str(i) for i in ints])\n",
    "            fout.write(f'{y_valid[i]},')\n",
    "            fout.write(pageStr)\n",
    "            fout.write('\\n')\n",
    "\n",
    "    with open(outfile_test, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for i in range(X_test.shape[0]):\n",
    "            ints = convertBinaryToInt(X_test[i,:,:])\n",
    "            pageStr = ' '.join([str(i) for i in ints])\n",
    "            fout.write(f'{y_test[i]},')\n",
    "            fout.write(pageStr)\n",
    "            fout.write('\\n')\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chunks_file = 'score_feat/augment/data.chunks256.augment3.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train_file = path/'augment/train256.augment3.csv'\n",
    "csv_valid_file = path/'augment/valid256.augment3.csv'\n",
    "csv_test_file = path/'augment/test256.augment3.csv'\n",
    "generateBootlegCSVFiles(save_chunks_file, composers, csv_train_file, csv_valid_file, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate csv files for evaluating on the original page classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSVFiles(bscore_pages_file, outfile_train, outfile_valid, outfile_test):\n",
    "    '''\n",
    "    Generates csv files for the original task of classifying full pages of music.\n",
    "    '''\n",
    "    \n",
    "    with open(bscore_pages_file, 'rb') as f:\n",
    "        d, pieceDirs_train, pieceDirs_valid, pieceDirs_test = pickle.load(f)\n",
    "    \n",
    "    generateFullPageCSV(d, pieceDirs_train, outfile_train)\n",
    "    generateFullPageCSV(d, pieceDirs_valid, outfile_valid)\n",
    "    generateFullPageCSV(d, pieceDirs_test, outfile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSV(d, keys, outfile):\n",
    "    \n",
    "    with open(outfile, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for pieceDir in keys: # e.g. score_feat/Bach/00748/\n",
    "            composer = pieceDir.split('/')[-3]\n",
    "            for m in d[pieceDir]: # d[pieceDir] -> list of binary bootleg score matrices, one per page\n",
    "                if m is not None:\n",
    "                    ints = convertBinaryToInt(m)\n",
    "                    textStr = ' '.join([str(i) for i in ints])\n",
    "                    fout.write(f'{composer},{textStr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train_file = path/'train.fullpage-augment7.csv'\n",
    "csv_valid_file = path/'valid.fullpage-augment7.csv'\n",
    "csv_test_file = path/'test.fullpage-augment7.csv'\n",
    "save_pages_file='score_feat/data.pages-augment7.pkl'\n",
    "generateFullPageCSVFiles(save_pages_file, csv_train_file, csv_valid_file, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also generate csv files to facilitate evaluating fixed-length classifiers on the full page classification task.  These classifiers will be applied to multiple windows of features, and the predictions will be averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEnsembleCSV(bscore_pages_file, chunkSz, outfile_test):\n",
    "    '''\n",
    "    Generates a csv file to facilitate evaluating fixed-length classifiers on the full page classification task.\n",
    "    Each line in the file corresponds to a fixed-length window of samples within a page.  The predictions from\n",
    "    all windows within a single page can then be averaged and evaluated.\n",
    "    '''\n",
    "    with open(bscore_pages_file, 'rb') as f:\n",
    "        d, pieceDirs_train, pieceDirs_valid, pieceDirs_test = pickle.load(f)\n",
    "        \n",
    "    with open(outfile_test, 'w') as fout:\n",
    "        fout.write('id,label,text\\n')\n",
    "        for pieceDir in pieceDirs_test: # e.g. score_feat/Bach/00748/\n",
    "            pieceID = pieceDir.split('/')[-2]\n",
    "            composer = pieceDir.split('/')[-3]\n",
    "            for i, m in enumerate(d[pieceDir]): # d[pieceDir] -> list of binary bootleg score matrices, one per page\n",
    "                if m is not None and m.shape[1] > 0:\n",
    "                    if m.shape[1] <= chunkSz: # only 1 window\n",
    "                        ints = convertBinaryToInt(m)\n",
    "                        textStr = ' '.join([str(i) for i in ints])\n",
    "                        idString = f'{pieceID}_{i}_0' # id: pieceID_pageIdx_chunkIdx\n",
    "                        fout.write(f'{idString},{composer},{textStr}\\n')\n",
    "                    else: # multiple windows\n",
    "                        numWindows = int(np.ceil(m.shape[1]/(chunkSz/2))) - 1 # hop by half the chunk size\n",
    "                        for j in range(numWindows - 1):\n",
    "                            startIdx = chunkSz // 2 * j\n",
    "                            endIdx = startIdx + chunkSz\n",
    "                            ints = convertBinaryToInt(m[:,startIdx:endIdx])\n",
    "                            textStr = ' '.join([str(i) for i in ints])\n",
    "                            idString = f'{pieceID}_{i}_{j}' # id: pieceID_pageIdx_chunkIdx\n",
    "                            fout.write(f'{idString},{composer},{textStr}\\n')\n",
    "                        # handle last window\n",
    "                        ints = convertBinaryToInt(m[:,-chunkSz:])\n",
    "                        textStr = ' '.join([str(i) for i in ints])\n",
    "                        idString = f'{pieceID}_{i}_{numWindows-1}' \n",
    "                        fout.write(f'{idString},{composer},{textStr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_test_file = path/'test.ensemble256-auugment7.csv'\n",
    "generateEnsembleCSV(save_pages_file,256, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for training and testing the Transformer-based models.  Instead of using decimal string representations, we represent each 62-bit bootleg score feature as a sequence of 8 one-byte characters.  Rather than generating these from scratch, we will simply convert the existing files to the new format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLMTrainFiles(indir, out_train, out_valid, val_frac=0.1):\n",
    "    \n",
    "    # split train/validation by file\n",
    "    filelist = sorted(glob.glob('{}/*.txt'.format(indir)))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(filelist)\n",
    "    endIdx = int(len(filelist) * (1-val_frac)) + 1\n",
    "    train_files = filelist[0:endIdx]\n",
    "    valid_files = filelist[endIdx:]\n",
    "    \n",
    "    # convert to binary string representation\n",
    "    convertToByteChars(train_files, out_train)\n",
    "    convertToByteChars(valid_files, out_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToByteChars(filelist, outfile):\n",
    "    '''\n",
    "    Split each 62-bit bootleg score feature into 8 bytes, and express each byte as a single character.\n",
    "    Consecutive bootleg score feature `words' will be separated by space.\n",
    "    '''\n",
    "    with open(outfile, 'w') as fout:\n",
    "        for infile in filelist:\n",
    "            with open(infile, 'r') as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    if len(line) > 0:\n",
    "                        if line == '</doc>':\n",
    "                            pass # skip\n",
    "                        else:\n",
    "                            converted = convertLineToCharSeq(line)\n",
    "                            fout.write(f'{converted}\\n')\n",
    "            fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLineToCharSeq(line):\n",
    "    ints = [int(p) for p in line.split()]\n",
    "    result = ' '.join([int2charseq(i) for i in ints])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2charseq(int64):\n",
    "    chars = ''\n",
    "    for i in range(8):\n",
    "        numshift = i * 8\n",
    "        charidx = (int64 >> numshift) & 255\n",
    "        chars += chr(19968 + charidx) # 19968 ensures that all chars are chinese characters (not newline, space, etc)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = path/'bpe_data'\n",
    "bpe_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target data\n",
    "lm_train_file = bpe_path/'bpe_lm_target_train-augmented.txt'\n",
    "lm_valid_file = bpe_path/'bpe_lm_target_valid-augmented.txt'\n",
    "dir_to_convert = path/'docs_target-augmented'\n",
    "generateLMTrainFiles(dir_to_convert, lm_train_file, lm_valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IMSLP data\n",
    "lm_train_file = bpe_path/'bpe_lm_imslp_train-augmented.txt'\n",
    "lm_valid_file = bpe_path/'bpe_lm_imslp_valid-augmented.txt'\n",
    "dir_to_convert = path/'docs_imslp-augmented'\n",
    "generateLMTrainFiles(dir_to_convert, lm_train_file, lm_valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSingleCSVFile(infile, outfile):\n",
    "    '''\n",
    "    Convert .csv file with decimal string representation of bootleg score features to\n",
    "    a .csv file with byte character representation.\n",
    "    '''\n",
    "    with open(infile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(outfile, 'w') as fout:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i==0: \n",
    "                fout.write(line) # header\n",
    "            else:\n",
    "                parts = line.strip().split(',')\n",
    "                feats = parts.pop()\n",
    "                charseq = convertLineToCharSeq(feats)\n",
    "                strToWrite = ','.join(parts) + ',' + charseq + '\\n'\n",
    "                fout.write(strToWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAllCSVFiles(indir, outdir):\n",
    "    assert indir != outdir\n",
    "    os.makedirs(outdir, exist_ok = True)\n",
    "    for infile in glob.glob(f'{indir}/*.csv'):\n",
    "        print(f'Converting {os.path.basename(infile)}')\n",
    "        basename = os.path.splitext(os.path.basename(infile))[0]\n",
    "        outfile = f'{outdir}/{basename}.char.csv'\n",
    "        convertSingleCSVFile(infile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertAllCSVFiles(str(path), str(bpe_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convertAllCSVFiles(str(path)+'/augment', str(bpe_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
