{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train a CNN classifier for the proxy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.torch_core import uniform_int\n",
    "import PIL\n",
    "import glob\n",
    "import re\n",
    "from eval_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory structure in ImageNet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent the chunked bootleg score features as binary images.  This will allow us to take advantage of the tools in the fastai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComposer2IndexMapping(feat_dir = 'score_feat'):\n",
    "    composers = []\n",
    "    for composerDir in sorted(glob.glob('{}/*/'.format(feat_dir))): # e.g. score_feat/Mozart/\n",
    "        composer = composerDir.split('/')[-2]\n",
    "        composers.append(composer)\n",
    "    c_to_i = {c:i for i, c in enumerate(composers)}\n",
    "    \n",
    "    return c_to_i, composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickleData(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDirectoryStructure(X, y, composers, dirType, outdir):\n",
    "    for c in composers:\n",
    "        composerDir = '{}/{}/{}/'.format(outdir, dirType, c)\n",
    "        os.makedirs(composerDir)\n",
    "    for i in range(X.shape[0]):\n",
    "        composer = composers[y[i]]\n",
    "        outfile = '{}/{}/{}/{}_{}.png'.format(outdir, dirType, composer, dirType, i)\n",
    "        saveToPng(X[i,:,:], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToPng(img, outfile):\n",
    "    img = np.uint8(img.astype(np.float) * 255)\n",
    "    pim = PIL.Image.fromarray(img)\n",
    "    pim.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksz = 64\n",
    "chunk_data_file = f'score_feat/data.chunks{chunksz}.pkl'\n",
    "chunk_data_dir = f'score_feat_imagenet_chunk{chunksz}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer2idx, composers = getComposer2IndexMapping()\n",
    "(X_train, y_train, pageinfo_train, X_valid, y_valid, pageinfo_valid, X_test, y_test, pageinfo_test) = loadPickleData(chunk_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDirectoryStructure(X_train, y_train, composers, 'train', chunk_data_dir)\n",
    "createDirectoryStructure(X_valid, y_valid, composers, 'valid', chunk_data_dir)\n",
    "createDirectoryStructure(X_test, y_test, composers, 'test', chunk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with the full page bootleg score features.  Since each page contains a different number of features, we will determine the maximum size and zero-pad all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFullPageTensorData(d, pieceDirs, c2i):\n",
    "    shape = calcTensorDims(d)\n",
    "    X = []\n",
    "    y = []\n",
    "    for pieceDir in pieceDirs: # e.g. score_feat/Bach/00756/\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        composeridx = c2i[composer]\n",
    "        for m in d[pieceDir]:\n",
    "            if m is not None:\n",
    "                mlen = m.shape[1]\n",
    "                curimg = np.zeros(shape, dtype=bool)\n",
    "                curimg[:,0:mlen] = m\n",
    "                X.append(curimg)\n",
    "                y.append(composeridx)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcTensorDims(d):\n",
    "    '''Determines the dimensions of the tensor needed to store all full page bootleg score features.'''\n",
    "    maxlen = 0\n",
    "    height = 0\n",
    "    for pieceDir in d:\n",
    "        for m in d[pieceDir]: # m: bootleg score matrix for a single page\n",
    "            if m is not None:\n",
    "                length = m.shape[1]\n",
    "                if length > maxlen:\n",
    "                    maxlen = length\n",
    "                    height = m.shape[0]\n",
    "    return height, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullpage_data_file = 'score_feat/data.pages.pkl'\n",
    "fullpage_data_dir = 'score_feat_imagenet_fullpage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, pieceDirs_train, pieceDirs_valid, pieceDirs_test = loadPickleData(fullpage_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = getFullPageTensorData(d, pieceDirs_train, composer2idx)\n",
    "X_valid, y_valid = getFullPageTensorData(d, pieceDirs_valid, composer2idx)\n",
    "X_test, y_test = getFullPageTensorData(d, pieceDirs_test, composer2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDirectoryStructure(X_train, y_train, composers, 'train', fullpage_data_dir)\n",
    "createDirectoryStructure(X_valid, y_valid, composers, 'valid', fullpage_data_dir)\n",
    "createDirectoryStructure(X_test, y_test, composers, 'test', fullpage_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform random crops and pitch shifts for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_shift(x, shift:uniform_int=0):\n",
    "    \"Cyclically shift the image in the vertical direction.\"\n",
    "    shifted = np.roll(np.array(x), shift, axis=1)\n",
    "    return tensor(np.ascontiguousarray(shifted))\n",
    "rand_shift = TfmPixel(_rand_shift)\n",
    "tfms_train = [crop(size=(62,64), row_pct=0.5, col_pct=(0,1)), rand_shift(shift=(-3,3))] # random crop + shift\n",
    "tfms_eval = [crop(size=(62,64), row_pct=0.5, col_pct=0.5)] # middle crop, no shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 512\n",
    "#data = ImageDataBunch.from_folder(chunk_data_dir, test='test', ds_tfms = (tfms_train, tfms_eval), bs=bs)\n",
    "data = ImageDataBunch.from_folder(chunk_data_dir, test='test', bs=bs) # no transforms to match other results\n",
    "getattr(data, 'train_dl').x.convert_mode = \"L\" # read images as grayscale\n",
    "getattr(data, 'valid_dl').x.convert_mode = \"L\"\n",
    "getattr(data, 'test_dl').x.convert_mode = \"L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, (8,3))\n",
    "        self.conv2 = nn.Conv2d(64, 300, (27,1))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(300, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "learner = Learner(data, model, metrics=[accuracy, FBeta(average='macro', beta=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(8, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('cnn_clas64')\n",
    "#learn.load('cnn_clas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up full-page classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle the full page classification task, we need to create wrapper models that can process variable length bootleg scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPageClassifier_cropped(nn.Module):\n",
    "    def __init__(self, pretrainedModel):\n",
    "        super(FullPageClassifier_cropped, self).__init__()\n",
    "        self.model = pretrainedModel\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, h, len_padded = x.shape\n",
    "        assert bs == 1 # must be 1, since each image will be cropped differently\n",
    "        assert c == 1\n",
    "        assert h == 62\n",
    "        assert len_padded >= 3\n",
    "        len_actual = torch.nonzero(x.sum(dim=2).squeeze() != 0)[-1,0]\n",
    "        out = self.model(x[:,:,:,0:len_actual])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPageClassifier_ensembled(nn.Module):\n",
    "    def __init__(self, pretrainedModel, chunklen, hop = None):\n",
    "        super(FullPageClassifier_ensembled, self).__init__()\n",
    "        self.model = pretrainedModel\n",
    "        self.chunklen = chunklen\n",
    "        self.hop = chunklen//2 if hop is None else hop\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, h, len_padded = x.shape\n",
    "        assert bs == 1 # must be 1, since each image will be cropped differently\n",
    "        assert c == 1\n",
    "        assert h == 62\n",
    "        assert len_padded >= 3\n",
    "        l = torch.nonzero(x.sum(dim=2).squeeze() != 0)[-1,0] + 1\n",
    "        startIdxs = np.arange(0, max(l-self.chunklen+1,1), self.hop)\n",
    "        assert len(startIdxs) > 0\n",
    "        bs_new = len(startIdxs)\n",
    "        x_crops = torch.zeros(bs_new, c, h, self.chunklen)\n",
    "        for i, startIdx in enumerate(startIdxs):\n",
    "            x_crops[i,:,:,:] = x[0,:,:,startIdx:startIdx+self.chunklen]\n",
    "        x_crops = x_crops.cuda()\n",
    "        out = self.model(x_crops).mean(dim=0, keepdims=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are some helper functions to evaluate these models on full page bootleg scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPagePrior_from_piecedirs(piece_dir_list, composer2idx):\n",
    "    targs = np.array([composer2idx[p.split('/')[-3]] for p in piece_dir_list]) # e.g. score_feat/Bach/00756/                   \n",
    "    numClasses = len(composer2idx)\n",
    "    counts = np.zeros(numClasses)\n",
    "    for i in range(numClasses):\n",
    "        counts[i] += np.sum(targs == i)\n",
    "    counts = counts / np.sum(counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cnn_fullpage(learner, priors):\n",
    "    '''Calculate evaluation metrics for CNN model on full page classification task.'''\n",
    "\n",
    "    # get predictions on full page bootleg scores                                                                              \n",
    "    probs, y = learner.get_preds(ds_type=DatasetType.Valid)\n",
    "\n",
    "    # ground truth labels                                                                                                      \n",
    "    gt = torch.from_numpy(learner.data.valid_ds.y.items)\n",
    "\n",
    "    # apply priors                                                                                                             \n",
    "    priors = torch.from_numpy(priors.reshape((1,-1)))\n",
    "    probs_with_priors = torch.mul(probs, priors)\n",
    "\n",
    "    # calc accuracy                                                                                                            \n",
    "    acc = accuracy(probs, gt).item()\n",
    "    acc_with_prior = accuracy(probs_with_priors, gt).item()\n",
    "\n",
    "    # calc macroF1                                                                                                             \n",
    "    f1 = macroF1(probs, gt)\n",
    "    f1_with_prior = macroF1(probs_with_priors, gt)\n",
    "\n",
    "    return (acc, acc_with_prior), (f1, f1_with_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the proxy task -- classifying fixed-length chunks of bootleg score features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=512\n",
    "data_test = ImageDataBunch.from_folder(chunk_data_dir, valid='test', bs=bs)\n",
    "getattr(data_test, 'train_dl').x.convert_mode = \"L\" # read images as grayscale\n",
    "getattr(data_test, 'valid_dl').x.convert_mode = \"L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate(data_test.valid_dl, metrics=[accuracy, FBeta(average='macro', beta=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the original task -- classifying pages of sheet music.  We can evaluate our models in two ways:\n",
    "- applying the model to a variable length sequence\n",
    "- applying the model to multiple fixed-length windows and averaging the predictions\n",
    "\n",
    "First we evaluate the model on variable length inputs.  Report results with and without applying priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1 # batch size must be 1 since each image is cropped differently                                                      \n",
    "data_test = ImageDataBunch.from_folder(fullpage_data_dir, valid='test', bs=bs)\n",
    "getattr(data_test, 'train_dl').x.convert_mode = \"L\" # read images as grayscale                                             \n",
    "getattr(data_test, 'valid_dl').x.convert_mode = \"L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fullpage_cropped = FullPageClassifier_cropped(learner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_single = Learner(data_test, model_fullpage_cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = calcPagePrior_from_piecedirs(pieceDirs_train + pieceDirs_valid, composer2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(acc, acc_with_prior), (f1, f1_with_prior) = evaluate_cnn_fullpage(learner_single, priors)\n",
    "(acc, acc_with_prior), (f1, f1_with_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the model by considering multiple fixed-length windows and averaging the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fullpage_ensembled = FullPageClassifier_ensembled(learner.model, chunksz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_ensembled = Learner(data_test, model_fullpage_ensembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(acc, acc_with_prior), (f1, f1_with_prior) = evaluate_cnn_fullpage(learner_ensembled, priors)\n",
    "(acc, acc_with_prior), (f1, f1_with_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate most commonly confused pairs at the fragment level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learner_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
